{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<center><h1> Load The Dataset </h1></center>\n",
   "id": "c19d01b24dbf9e27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For this Small Language Model we will use a dataset called **TinyStories**. It is a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by **GPT-3.5** and **GPT-4**. We can get it from [Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories).",
   "id": "ab1dcd5aba741ce1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T03:25:16.734357Z",
     "start_time": "2025-07-13T03:25:14.081229Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install datasets",
   "id": "ada5359d6174122e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/miniconda3/lib/python3.12/site-packages (4.0.0)\r\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (3.16.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (2.1.3)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (18.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.4)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (0.33.4)\r\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T03:25:39.893442Z",
     "start_time": "2025-07-13T03:25:36.984484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ],
   "id": "1145242cc83d5a40",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<center><h1>Tokenize The Dataset</h1></center>",
   "id": "cb86728c64f36dc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Tokenization** is the process of breaking down sequence of text into smaller units called tokens.\n",
    "The tokenizer we will use is **GPT-2 sub-word Tokenizer** which uses **Bypair Encodding(BPE)**.\n",
    "\n",
    "- Dataset -> Tokenizer -> Tokens -> TokenID"
   ],
   "id": "d3180230f2568a10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**In this step, we will:**\n",
    "\n",
    "1. Tokenize the dataset into `tokenIDs`.\n",
    "2. Create two binary files:\n",
    "   - `Train.bin` with **2,000,000** rows\n",
    "   - `Validation.bin` with **22,000** rows\n",
    "3. Store all the token IDs in a single .bin file.\n",
    "   - This will store the tokenIDs in disk storage, no in RAM.\n",
    "      - Fast loading during training\n",
    "      - No need to re-tokenize\n",
    "\n",
    "These files will store the `tokenIDs` generated from the entire dataset."
   ],
   "id": "f5cc8055eb3188f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T03:44:55.102909Z",
     "start_time": "2025-07-13T03:38:21.751723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install tiktoken #Tiktoken is a library from OpenAI from which we can get different tokenizers.\n",
    "import tiktoken\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(example['text']) #encode_ordinary ignores any special tokens\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "if not os.path.exists('train.bin'):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc= 'tokenizing the splits',\n",
    "        num_proc= 8,\n",
    "    )\n",
    "\n",
    "#Concatenate all the ids in each dataset into one large file which will be used for training\n",
    "for split, dset in tokenized.items():\n",
    "    arr_len = np.sum(dset['len'], dtype= np.uint64)\n",
    "    filename = f'{split}.bin'\n",
    "    dtype = np.uint16 #can do since enc.max_token_value == 50256 is < 2**16\n",
    "    arr = np.memmap(filename, dtype = dtype, mode = 'w+', shape= (arr_len,))\n",
    "    total_batches = 1024\n",
    "\n",
    "    idx = 0\n",
    "    for batch_idx in tqdm(range(total_batches), desc=f'wrriting {filename}'):\n",
    "        #Batch samples together for faster write\n",
    "        batch = dset.shard(num_shards = total_batches, index = batch_idx, contiguous = True).with_format('numpy')\n",
    "        arr_batch = np.concatenate(batch['ids'])\n",
    "        #Write into map\n",
    "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "        idx += len(arr_batch)\n",
    "    arr.flush()"
   ],
   "id": "3eca109e0916baaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/miniconda3/lib/python3.12/site-packages (0.9.0)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/miniconda3/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/miniconda3/lib/python3.12/site-packages (from tiktoken) (2.32.4)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wrriting train.bin: 100%|██████████| 1024/1024 [06:08<00:00,  2.78it/s]\n",
      "wrriting validation.bin: 100%|██████████| 1024/1024 [00:05<00:00, 185.68it/s]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<center><h1>Input-Output Batches</h1><center>",
   "id": "20f0992a6eb664a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Input Batch**: A group of tokenized sequences fed into the model simultaneously during training or inference. Each sequence represents a segment of text, and the entire batch enables efficient parallel processing.\n",
    "\n",
    "- **Output Batch**: The corresponding group of target sequences that the model is trained to predict, typically formed by shifting the input sequences one token to the left. These are used to compute the loss during training."
   ],
   "id": "3e1bf7786a2342ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:08:46.726258Z",
     "start_time": "2025-07-13T09:08:46.720705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = np.menmap('train.bin', dtype = np.unit16, mode = 'r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype = np.uint16, mode = 'r')\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "id": "fc4d3c2fce9e9218",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<center><h1>SLM Model Architechture</h1><center>",
   "id": "b03f0d0709783cbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T11:37:47.461398Z",
     "start_time": "2025-07-15T11:37:44.946866Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install torch torchvision torchaudio",
   "id": "13c4df822a183f5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/miniconda3/lib/python3.12/site-packages (2.7.1)\r\n",
      "Requirement already satisfied: torchvision in /opt/miniconda3/lib/python3.12/site-packages (0.22.1)\r\n",
      "Requirement already satisfied: torchaudio in /opt/miniconda3/lib/python3.12/site-packages (2.7.1)\r\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.12/site-packages (from torch) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/miniconda3/lib/python3.12/site-packages (from torch) (4.14.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.12/site-packages (from torch) (68.2.2)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/miniconda3/lib/python3.12/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.12/site-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.12/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.12/site-packages (from torch) (2025.3.0)\r\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.12/site-packages (from torchvision) (2.1.3)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from torchvision) (11.0.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T11:38:58.829567Z",
     "start_time": "2025-07-15T11:38:58.806525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim))\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'Scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                 .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout\n",
    "            if self.training else 0.0, is_casual=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "    class Block(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.ln1 =LayerNorm(config.n_embd, config.bias)\n",
    "            self.attn = CasualSelfAttention(config)\n",
    "            self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "            self.mlp = MLP(config)\n",
    "        def forward(self, x):\n",
    "            x = x + self.attn(self.ln1(x))\n",
    "            x = x + self.mlp(self.ln2(x))\n",
    "            return x\n",
    "\n",
    "@dataclass\n",
    "class GPTconfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float == 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arrange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok.emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx = Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v [:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ],
   "id": "711587954f9b0257",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<center><h1>Loss Function</h1></center>",
   "id": "116da2dd89b8fcab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A loss function quantifies the difference between the predicted output and the true label (actual value). It returns a number (called **loss**) which the model tries to **minimize** during training.\n",
    "\n",
    "- **Lower loss = Better predictions**\n",
    "- **Higher loss = Poor predictions**"
   ],
   "id": "98e2843c80344e0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "id": "2f7de3ed13cdea09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
