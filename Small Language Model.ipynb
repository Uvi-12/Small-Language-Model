{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<center><h1> Load The Dataset </h1></center>\n",
   "id": "c19d01b24dbf9e27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For this Small Language Model we will use a dataset called **TinyStories**. It is a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by **GPT-3.5** and **GPT-4**. We can get it from [Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories).",
   "id": "ab1dcd5aba741ce1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T03:25:16.734357Z",
     "start_time": "2025-07-13T03:25:14.081229Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install datasets",
   "id": "ada5359d6174122e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/miniconda3/lib/python3.12/site-packages (4.0.0)\r\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (3.16.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (2.1.3)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (18.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.4)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (0.33.4)\r\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T03:25:39.893442Z",
     "start_time": "2025-07-13T03:25:36.984484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ],
   "id": "1145242cc83d5a40",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<center><h1>Tokenize The Dataset</h1></center>",
   "id": "cb86728c64f36dc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Tokenization** is the process of breaking down sequence of text into smaller units called tokens.\n",
    "The tokenizer we will use is **GPT-2 sub-word Tokenizer** which uses **Bypair Encodding(BPE)**.\n",
    "\n",
    "- Dataset -> Tokenizer -> Tokens -> TokenID"
   ],
   "id": "d3180230f2568a10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**In this step, we will:**\n",
    "\n",
    "1. Tokenize the dataset into `tokenIDs`.\n",
    "2. Create two binary files:\n",
    "   - `Train.bin` with **2,000,000** rows\n",
    "   - `Validation.bin` with **22,000** rows\n",
    "3. Store all the token IDs in a single .bin file.\n",
    "   - This will store the tokenIDs in disk storage, no in RAM.\n",
    "      - Fast loading during training\n",
    "      - No need to re-tokenize\n",
    "\n",
    "These files will store the `tokenIDs` generated from the entire dataset."
   ],
   "id": "f5cc8055eb3188f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T03:44:55.102909Z",
     "start_time": "2025-07-13T03:38:21.751723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install tiktoken #Tiktoken is a library from OpenAI from which we can get different tokenizers.\n",
    "import tiktoken\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(example['text']) #encode_ordinary ignores any special tokens\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "if not os.path.exists('train.bin'):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc= 'tokenizing the splits',\n",
    "        num_proc= 8,\n",
    "    )\n",
    "\n",
    "#Concatenate all the ids in each dataset into one large file which will be used for training\n",
    "for split, dset in tokenized.items():\n",
    "    arr_len = np.sum(dset['len'], dtype= np.uint64)\n",
    "    filename = f'{split}.bin'\n",
    "    dtype = np.uint16 #can do since enc.max_token_value == 50256 is < 2**16\n",
    "    arr = np.memmap(filename, dtype = dtype, mode = 'w+', shape= (arr_len,))\n",
    "    total_batches = 1024\n",
    "\n",
    "    idx = 0\n",
    "    for batch_idx in tqdm(range(total_batches), desc=f'wrriting {filename}'):\n",
    "        #Batch samples together for faster write\n",
    "        batch = dset.shard(num_shards = total_batches, index = batch_idx, contiguous = True).with_format('numpy')\n",
    "        arr_batch = np.concatenate(batch['ids'])\n",
    "        #Write into map\n",
    "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "        idx += len(arr_batch)\n",
    "    arr.flush()"
   ],
   "id": "3eca109e0916baaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/miniconda3/lib/python3.12/site-packages (0.9.0)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/miniconda3/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/miniconda3/lib/python3.12/site-packages (from tiktoken) (2.32.4)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wrriting train.bin: 100%|██████████| 1024/1024 [06:08<00:00,  2.78it/s]\n",
      "wrriting validation.bin: 100%|██████████| 1024/1024 [00:05<00:00, 185.68it/s]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<center><h1>Input-Output Batches</h1><center>",
   "id": "20f0992a6eb664a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Input Batch**: A group of tokenized sequences fed into the model simultaneously during training or inference. Each sequence represents a segment of text, and the entire batch enables efficient parallel processing.\n",
    "\n",
    "- **Output Batch**: The corresponding group of target sequences that the model is trained to predict, typically formed by shifting the input sequences one token to the left. These are used to compute the loss during training."
   ],
   "id": "3e1bf7786a2342ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:08:46.726258Z",
     "start_time": "2025-07-13T09:08:46.720705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = np.menmap('train.bin', dtype = np.unit16, mode = 'r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype = np.uint16, mode = 'r')\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "id": "fc4d3c2fce9e9218",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
