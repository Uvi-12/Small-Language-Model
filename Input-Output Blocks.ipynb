{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Input Batch**: A group of tokenized sequences fed into the model simultaneously during training or inference. Each sequence represents a segment of text, and the entire batch enables efficient parallel processing.\n",
    "\n",
    "- **Output Batch**: The corresponding group of target sequences that the model is trained to predict, typically formed by shifting the input sequences one token to the left. These are used to compute the loss during training."
   ],
   "id": "402c69e7912b395"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = np.menmap('train.bin', dtype = np.unit16, mode = 'r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype = np.uint16, mode = 'r')\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
